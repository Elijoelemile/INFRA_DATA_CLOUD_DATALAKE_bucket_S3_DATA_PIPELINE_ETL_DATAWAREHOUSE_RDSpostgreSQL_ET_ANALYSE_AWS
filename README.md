# ğŸ“Š Projet #2 â€“ Mise en Place et Optimisation d'une Infrastructure Data sur le Cloud

## ğŸ‘¤ Auteur
**ESSONO-NZOGHE Eli-joÃ«l-emile** â€“ MastÃ¨re 1 Architecte en Intelligence Artificielle **Promo AIA01**  
**AnnÃ©e 1 - L'Ã‰cole MultimÃ©dia**

---

## ğŸ§­ Objectif du projet

Ce projet consiste Ã  concevoir une infrastructure Data moderne en combinant :
- un **pipeline ETL local** pour collecter et transformer des donnÃ©es issues de **scraping** et **APIs**,
- un **Data Lake sur AWS S3** pour stocker les donnÃ©es brutes,
- un **Data Warehouse sur RDS (PostgreSQL)** pour les donnÃ©es nettoyÃ©es,
- et des **visualisations interactives** via Plotly et/ou Bokeh.

---

## ğŸ—‚ï¸ Arborescence du projet

```
â”œâ”€â”€ etl/                # DonScripts de scraping, API, traitement, upload vers AWS
â”œâ”€â”€ infrastructure/     # RequÃªtage et envoi de donnÃ©es sur Data Lake et Data warehouse(AWS)
â”œâ”€â”€ Livrable/           # Ensemble du travail effectuÃ©
â”œâ”€â”€ notebook/           # Analyses exploratoires (Jupyter)
â”œâ”€â”€ venv/               # Environnement virtuel
â”œâ”€â”€ .env/               # Variables d'environnement
â”œâ”€â”€ .gitignore/         # Ignorance de certains fichiers pour le Github
â”œâ”€â”€ main.py             # Orchestration de toute l'application(Pas encore achevÃ©)
â”œâ”€â”€ README.md           # Ce fichier
â”œâ”€â”€ requirements.txt    # DÃ©pendances Python
```

---

## ğŸ”§ Pipeline ETL (local)

### Ã‰tapes :
1. **Scraping** d'offres d'emploi via `etl/extractsraping.py`
2. **RequÃªte API** (Remotive) via `etl/extractapi.py`
3. **Nettoyage et transformation** des donnÃ©es (Pandas)
4. **Enregistrement** :
   - DonnÃ©es brutes âœ `S3`
   - DonnÃ©es transformÃ©es âœ `RDS PostgreSQL`

### ExÃ©cution :
```bash
python etl/main_etl.py
```

---

## â˜ï¸ Infrastructure Cloud (AWS)

### Services utilisÃ©s :
- **S3** : stockage brut (Data Lake)
- **RDS PostgreSQL** : stockage structurÃ© (Data Warehouse)
- **IAM** : sÃ©curitÃ© des accÃ¨s (via rÃ´les et policies)
- **(Sans Glue ni Lambda)** â€“ Pipeline entiÃ¨rement local

ğŸ‘‰ Voir le dossier [`infrastructure/`](infrastructure/) pour :
- Architecture Data Clouud(AWS) (`diagramme_infra.pdf`)
- scripts d'envoie de donnÃ©es sur S3 et RDS

---

## ğŸ“Š Analyse

- Librairies utilisÃ©es : `Plotly`, `Pandas`
- Analyses rÃ©alisÃ©es :
  - Top 10 des jobs les plus demandÃ©s
  - Top 10 des jobs moyennement demandÃ©s
  - Top 10 des jobs les moins demandÃ©s
  - Top 5 des localisations avec le plus d'offres

---

## ğŸ’¾ AccÃ¨s aux donnÃ©es

- DonnÃ©es brutes accessibles dans S3 (`my-datalakeeli`)
- DonnÃ©es nettoyÃ©es dans la base PostgreSQL (`data_cleaned_20250627.csv`)
- Notebooks disponibles dans [`notebook/`](notebook/)

---

## ğŸ” SÃ©curitÃ© & bonnes pratiques

- S3 avec **chiffrement activÃ©**
- RDS accessible uniquement depuis une IP spÃ©cifique
- IAM configurÃ© pour limiter les droits
- CloudTrail activÃ© pour audit
- GitHub utilisÃ© pour le versionnage (commits frÃ©quents)

---

## ğŸ“ DÃ©pendances

Liste dans `requirements.txt`, ex :
```
requests
pandas
sqlalchemy
psycopg2
plotly
bokeh
python-dotenv
```

---

## ğŸ“Œ Auteur & contact

**Nom PrÃ©nom**  
Promo AIA01  
Email : nzoghessono@gmail.com  
