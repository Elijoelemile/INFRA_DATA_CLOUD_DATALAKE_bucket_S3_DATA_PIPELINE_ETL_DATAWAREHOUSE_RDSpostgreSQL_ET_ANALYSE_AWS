# ğŸ“Š Projet #2 â€“ Mise en Place et Optimisation d'une Infrastructure Data sur le Cloud

## ğŸ‘¤ Auteur
**Nom PrÃ©nom** â€“ Promo AIA01  
**AnnÃ©e 1 - L'Ã‰cole MultimÃ©dia**

---

## ğŸ§­ Objectif du projet

Ce projet consiste Ã  concevoir une infrastructure Data moderne en combinant :
- un **pipeline ETL local** pour collecter et transformer des donnÃ©es issues de **scraping** et **APIs**,
- un **Data Lake sur AWS S3** pour stocker les donnÃ©es brutes,
- un **Data Warehouse sur RDS (PostgreSQL)** pour les donnÃ©es nettoyÃ©es,
- et des **visualisations interactives** via Plotly et/ou Bokeh.

---

## ğŸ—‚ï¸ Arborescence du projet

```
â”œâ”€â”€ data/               # DonnÃ©es locales (brutes et transformÃ©es)
â”œâ”€â”€ etl/                # Scripts de scraping, API, traitement, upload vers AWS
â”œâ”€â”€ dashboard/          # Visualisations et dashboards interactifs
â”œâ”€â”€ notebooks/          # Analyses exploratoires (Jupyter)
â”œâ”€â”€ infrastructure/     # Configs AWS, scripts CLI, diagrammes, IAM
â”œâ”€â”€ README.md           # Ce fichier
â”œâ”€â”€ requirements.txt    # DÃ©pendances Python
```

---

## ğŸ”§ Pipeline ETL (local)

### Ã‰tapes :
1. **Scraping** d'offres d'emploi via `etl/scraper_wttj.py`
2. **RequÃªte API** (Adzuna ou PÃ´le Emploi) via `etl/api_adzuna.py`
3. **Nettoyage et transformation** des donnÃ©es (Pandas)
4. **Enregistrement** :
   - DonnÃ©es brutes âœ `S3`
   - DonnÃ©es transformÃ©es âœ `RDS PostgreSQL`

### ExÃ©cution :
```bash
python etl/main_etl.py
```

---

## â˜ï¸ Infrastructure Cloud (AWS)

### Services utilisÃ©s :
- **S3** : stockage brut (Data Lake)
- **RDS PostgreSQL** : stockage structurÃ© (Data Warehouse)
- **IAM** : sÃ©curitÃ© des accÃ¨s (via rÃ´les et policies)
- **CloudTrail** : journalisation des accÃ¨s
- **(Sans Glue ni Lambda)** â€“ Pipeline entiÃ¨rement local

ğŸ‘‰ Voir le dossier [`infrastructure/`](infrastructure/) pour :
- diagramme dâ€™architecture (`diagramme_infra.png`)
- scripts CLI de crÃ©ation S3 et RDS
- configurations IAM JSON
- captures Ã©cran (CloudTrail)

---

## ğŸ“Š Visualisation & Analyse

- Dashboard interactif : `dashboard/jobs_dashboard.py`
- Librairies utilisÃ©es : `Plotly`, `Bokeh`, `Seaborn`, `Pandas`
- Analyses rÃ©alisÃ©es :
  - RÃ©partition gÃ©ographique des offres
  - Technologies les plus demandÃ©es
  - Ã‰volution temporelle des publications

---

## ğŸ’¾ AccÃ¨s aux donnÃ©es

- DonnÃ©es brutes accessibles dans S3 (`data-lake-emploi`)
- DonnÃ©es nettoyÃ©es dans la base PostgreSQL (`jobs_analytics_db`)
- Notebooks disponibles dans [`notebooks/`](notebooks/)

---

## ğŸ” SÃ©curitÃ© & bonnes pratiques

- S3 avec **chiffrement activÃ©**
- RDS accessible uniquement depuis une IP spÃ©cifique
- IAM configurÃ© pour limiter les droits
- CloudTrail activÃ© pour audit
- GitHub utilisÃ© pour le versionnage (commits frÃ©quents)

---

## ğŸ“ DÃ©pendances

Liste dans `requirements.txt`, ex :
```
requests
pandas
sqlalchemy
psycopg2
plotly
bokeh
python-dotenv
```

---

## ğŸ“ Livraison

Le projet est livrÃ© sous forme dâ€™une archive :
```
prenom_nom_projet2_AIA01.zip
```
Incluant :
- tous les scripts, notebooks, dashboards
- le dÃ©pÃ´t complet GitHub
- une documentation complÃ¨te (`infrastructure/`)
- une prÃ©sentation PowerPoint du projet

---

## ğŸ“Œ Auteur & contact

**Nom PrÃ©nom**  
Promo AIA01  
Email : nzoghessono@gmail.com  
